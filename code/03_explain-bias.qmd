---
title: "explain-bias"
format: html
editor: visual
---

# Installing / loading packages

First, clear workspace:

```{r}
rm(list=ls())
```

```{r}
wd <- "/Volumes/DEBIAS"
```

```{r}
#| warning: false
#| message: false
# library necessary packages
library(xgboost)
library(parallel)
library(recommenderlab)
library(doParallel)
library(rsample)
library(tidyverse)
library(sf)
library(moments)
library(SHAPforxgboost)
library(shapviz)
library(pdp)
library(rsample)
library(foreach)
library(patchwork)
library(viridis)
library(scales)
library(tidyplots)
```

# Set theme

```{r}
#| include = FALSE
# clear memory and adds tufte templates for plots and maps
source("../code/style/data-visualisation_theme.R")
```

# Data

## Data import

This first line defines the data source to be analysed. We have three options:

-   `twitter`

-   `fb_stt`

-   `fb_tts`

-   `mapp1`

-   `mapp2`

```{r}
dfd <- "mapp2"
```

The next code chunk can be run using spatial weights and nonspatial weights options:

-   distance band to contain at least one neighbour: `active_population_bias_lad_w_db.csv`

-   fixed optimal distance band based on GWR cross-validation: `active_population_bias_lad_w_fbw.csv`

-   k-nearest neighbour: `active_population_bias_lad_w_knn.csv`

-   queen: `active_population_bias_lad_w_queen.csv`

-   nonspatial: `active_population_bias_lad.csv`

df_var_lbl \<- read_csv(paste0(wd,"/data/inputs/census/census2021-ts/dictionary.csv")) %\>%

filter(!grepl("excluded", .\$group, ignore.case = TRUE))

```{r}
# Read in data

# census
df <- read.csv(paste0(wd, "/data/inputs/census/census2021-ts/combined-data-ltla.csv")) 

# census labels
df_var_lbl <- read_csv(paste0(wd,"/data/inputs/census/census2021-ts/dictionary.csv")) %>% 
  filter(!grepl("excluded", .$group, ignore.case = TRUE))

# administrative boundaries
lda_sdf <- st_read("/Volumes/DEBIAS/data/inputs/geographies/boundaries/LAD_Dec_2021_GB_BFC_2022_simplified.gpkg")

# digital footprint mobility data
if (dfd == "twitter") {
  df_bias <- read.csv(paste0(wd, "/data/outputs/twitter/monthly/populations/active-population-bias/active_population_bias_lad.csv")) %>% rename("geography.code" = "code") %>% dplyr::filter(name != "City of London")
  } else if (dfd == "fb_tts") {
  df_bias <- read.csv(paste0(wd, "/data/outputs/fb/census-month/populations/tts/active-population-bias/active_population_bias_lad.csv")) %>% rename("geography.code" = "code")
  } else if (dfd == "fb_stt") {
  df_bias <- read.csv(paste0(wd, "/data/outputs/fb/census-month/populations/stt/active-population-bias/active_population_bias_lad.csv")) %>% rename("geography.code" = "code")
  }  else if (dfd == "mapp1") {
  df_bias <- read.csv(paste0(wd, "/data/outputs/mapp1/census-month/populations/active-population-bias/active_population_bias_lad.csv")) %>% rename("geography.code" = "code") %>% dplyr::filter(name != "City of London")
  }  else if (dfd == "mapp2") {
  df_bias <- read.csv(paste0(wd, "/data/outputs/mapp2/census-month/populations/active-population-bias/active_population_bias_lad.csv")) %>% rename("geography.code" = "code")
  }

# add spatial weights to the label data frame if they are included 
if ("bias_w" %in% names(df_bias)) {
  df_var_lbl[46, ] <- list("bias_w", "spatial_lag_bias", "Spatial Lag", 
                      "spatial_lag_bias", "spatial_lag_bias", "bias_w")
}
```

## Data wrangling

### Remove areas in Scotland and Northern Ireland

```{r}
# from boundaries
lda_sdf <- lda_sdf %>%
  filter(!grepl("^[SN]", `code`))
```

## Select input predictors

```{r}
df <- df %>%
  select(all_of(df_var_lbl$var_label5))
```

### Extract centroids

```{r}
# extract centroids
centroids_df <- st_centroid(lda_sdf) %>% 
  st_coordinates() %>% 
  as.data.frame() %>% 
  rowid_to_column("row_id")

# transfer area ids
centroids_with_ids <- lda_sdf %>%
  st_set_geometry(NULL) %>%              # Remove geometry to avoid issues with join
  rowid_to_column("row_id") %>%         # Create a temporary row_id for merging
  cbind(centroids_df) 

# remove row_id and rename x,y
centroids_with_ids <- centroids_with_ids %>% 
  select(-c(row_id)) %>% 
  rename(
    longitude = "X",
    latitude = "Y"
  )

# join
df_extended <- full_join(df, centroids_with_ids, by = join_by(geography.code == code) ) 
```

If spatial weights are included as a feature:

```{r}
if ("bias_w" %in% names(df_bias)) {
  df_extended <- merge(df_extended, df_bias[, c("geography.code", "bias_w")], by = "geography.code")
}
```

```{r}
# select numeric columns and standardize them
df_standardised <- df_extended %>%
  dplyr::select(-c(date)) %>% 
  mutate(across(where(is.numeric), ~ (. - mean(.)) / sd(.)))
```

```{r}
input_df <- merge(df_standardised, df_bias[, c("geography.code", "bias")], by = "geography.code")
```

```{r}
print(which(is.na(input_df), arr.ind = TRUE))

input_df <- input_df %>% 
  select(-longitude, -latitude) %>% 
  drop_na()
```

```{r}
rm(lda_sdf, df_standardised, df_extended)
```

## 2.2 Setting up model

```{r}
# Proportion of time-series data to assign to training vs testing
train_prop <- 0.7

# Proportion of training data which should be used to train the hyper-parameters
train_sample_prop <- 0.7

# Proportion of training data which should be used to validate the hyper-parameters
test_sample_prop <- 1

# Should the training and testing date be cut randomly ("random") or temporally ("temporal")?
cut <- "random"

# Independent variables
xvars <- colnames(input_df[, 4:ncol(input_df)-1])

# Final model inputs
y <- "bias"
x <- c(xvars)
```

```{r}
xvars
```

## 3.2 Defining training and test data sets

```{r}
set.seed(123)
split_df <- initial_split(input_df, prop = train_sample_prop)
train_sample <- training(split_df)
test_sample  <- testing(split_df)
```

features

```{r}
# variable names
features <- xvars

# Create the treatment plan from the training data
treatplan <- vtreat::designTreatmentsZ(train_sample, 
                                       features, 
                                       verbose = FALSE)

# Get the "clean" variable names from the scoreFrame
new_vars <- treatplan %>%
  magrittr::use_series(scoreFrame) %>%        
  dplyr::filter(code %in% c("clean", "lev")) %>% 
  magrittr::use_series(varName)  

# Prepare the training data
features_train <- vtreat::prepare(treatplan, 
                                  train_sample, 
                                  varRestriction = new_vars) %>% 
  as.matrix()

response_train <- train_sample[,y]

# Prepare the test data
features_test <- vtreat::prepare(treatplan, test_sample, varRestriction = new_vars) %>% as.matrix()
response_test <- test_sample[, y]

# dimensions of one-hot encoded data
dim(features_train)
dim(features_test)
```

# Training

Set up parameters for model. Use of default parameters to train a basic 10-fold cross validated XGBoost model with 1,000 trees. THe default parameters are:

-   learning rate (eta): 0.3
-   tree depth (max_depth): 6
-   minimum node size (min_child_weight): 1
-   percent of training data to sample for each tree (subsample --\> equivalent to gbm's bag.fraction): 100%
-   early_stopping_rounds = 10 \# stop if no improvement for 10 consecutive

```{r}
# reproducibility
set.seed(123)

xgb.fit1 <- xgb.cv(
  data = features_train,
  label = response_train,
  nrounds = 1000,
  nfold = 10,
  objective = "reg:squarederror",  # for regression models
  verbose = 0,               # silent,
#  early_stopping_rounds = 10
)
```

```{r}
# get number of trees that minimize error
xgb.fit1$evaluation_log %>%
  dplyr::summarise(
    ntrees.train = which(train_rmse_mean == min(train_rmse_mean))[1],
    rmse.train   = min(train_rmse_mean),
    ntrees.test  = which(test_rmse_mean == min(test_rmse_mean))[1],
    rmse.test   = min(test_rmse_mean),
  )

# plot error vs number trees
ggplot(xgb.fit1$evaluation_log) +
  geom_line(aes(iter, train_rmse_mean), color = "red") +
  geom_line(aes(iter, test_rmse_mean), color = "blue")
```

We can see that the training error continues to decrease to 778 trees where the RMSE nearly reaches 0.0009509059; however, the cross validated error reaches a minimum RMSE of 1650 with only 14 trees.

## Tuning

To tune the XGBoost model we pass parameters as a list object to the params argument. The most common parameters include:

-   eta:controls the learning rate
-   max_depth: tree depth
-   min_child_weight: minimum number of observations required in each terminal node
-   subsample: percent of training data to sample for each tree
-   colsample_bytrees: percent of columns to sample from for each tree

We perform a large search grid, and create our hyperparameter search grid along with columns to dump our results in.

```{r}
# create hyperparameter grid
hyper_grid <- expand.grid(
  #nrounds = c(50, 100, 150),          # Number of boosting rounds
  eta = c(.01, .05, .1, .3),           # Learning rate
  #gamma = c(0, 1, 5),                 # Minimum loss reduction
  max_depth = c(1, 3, 5, 7),           # Maximum depth of a tree
  min_child_weight = c(1, 3, 5, 7),    # Minimum sum of instance weight
  subsample = c(.65, .8, 1),           # Fraction of rows per tree
  colsample_bytree = c(.8, .9, 1),     # Fraction of columns per tree
  lambda = c(0, 0.5, 1),               # L2 regularization term (Ridge)
  alpha = c(0, 0.5, 1),                # L1 regularization term (LASSO)
  optimal_trees = 0,               # a place to dump results
  min_RMSE = 0                     # a place to dump results
)

nrow(hyper_grid)
```

## Parallelised code

We apply a loop procedure to loop through and apply a XGBoost model for each hyperparameter combination and dump the results in the hyper_grid data frame. This is done in a paralellised way.

```{r}

# Measure the time taken for parallel grid search
time_taken <- system.time({
  
# Set up parallel backend with the number of cores (adjust according to your system)
num_cores <- detectCores() - 1  # to leave one core free
cl <- makeCluster(num_cores)
registerDoParallel(cl)

# Parallelized grid search
results <- foreach(i = 1:nrow(hyper_grid), .combine = rbind, .packages = "xgboost") %dopar% {
  
  # create parameter list
  params <- list(
    eta = hyper_grid$eta[i],
    #gamma = hyper_grid$gamma[i],
    max_depth = hyper_grid$max_depth[i],
    min_child_weight = hyper_grid$min_child_weight[i],
    subsample = hyper_grid$subsample[i],
    colsample_bytree = hyper_grid$colsample_bytree[i],
    lambda = hyper_grid$lambda[i],
    alpha = hyper_grid$alpha[i]
  )
  
  # reproducibility
  set.seed(123)
  
  # train model
  xgb.tune <- xgb.cv(
    params = params,
    data = features_train,
    label = response_train,
    nrounds = 5000,
    nfold = 10,
    objective = "reg:squarederror",  # for regression models
    verbose = 0,                     # silent,
    early_stopping_rounds = 10       # stop if no improvement for 10 consecutive trees
  )
  
  # collect the optimal number of trees and minimum RMSE
  optimal_trees <- which.min(xgb.tune$evaluation_log$test_rmse_mean)
  min_RMSE <- min(xgb.tune$evaluation_log$test_rmse_mean)
  
  # return as a row (with hyperparameters and results)
  return(data.frame(
    eta = hyper_grid$eta[i],
    #gamma = hyper_grid$gamma[i],
    max_depth = hyper_grid$max_depth[i],
    min_child_weight = hyper_grid$min_child_weight[i],
    subsample = hyper_grid$subsample[i],
    colsample_bytree = hyper_grid$colsample_bytree[i],
    lambda = hyper_grid$lambda[i],
    alpha = hyper_grid$alpha[i],
    optimal_trees = optimal_trees,
    min_RMSE = min_RMSE
  ))
}

# Convert to a data frame if not already (should be)
results <- as.data.frame(results)

# Stop the cluster after processing
stopCluster(cl)
registerDoSEQ()  # Reset back to sequential processing

# Display the results
print(results)

})

# Print the time taken
print(time_taken)
```

## Non-parallelised code

::: callout-warning
This may take a while so leave it running overnight!
:::

```{r}

# # Measure the time taken for parallel grid search
# time_taken <- system.time({
#   
# # grid search 
# for(i in 1:nrow(hyper_grid)) {
# 
#   # create parameter list
#   params <- list(
#     eta = hyper_grid$eta[i],
#     max_depth = hyper_grid$max_depth[i],
#     min_child_weight = hyper_grid$min_child_weight[i],
#     subsample = hyper_grid$subsample[i],
#     colsample_bytree = hyper_grid$colsample_bytree[i]
#   )
# #
# #   # reproducibility
#   set.seed(123)
# 
#   # train model
#   xgb.tune <- xgb.cv(
#     params = params,
#     data = features_train,
#     label = response_train,
#     nrounds = 5000,
#     nfold = 10,
#     objective = "reg:squarederror",  # for regression models
#     verbose = 0,               # silent,
#     early_stopping_rounds = 10 # stop if no improvement for 10 consecutive trees
#   )
# #
# #   # add min training error and trees to grid
#    hyper_grid$optimal_trees[i] <- which.min(xgb.tune$evaluation_log$test_rmse_mean)
#    hyper_grid$min_RMSE[i] <- min(xgb.tune$evaluation_log$test_rmse_mean)
# }
# 
# })
# 
# # Print the time taken
# print(time_taken)
# 
# hyper_grid %>%
#   dplyr::arrange(min_RMSE) %>%
#   head(10)
# 
# print(hyper_grid)
```

```{r}

if (dfd == "twitter"){
  write_csv(results, paste0(wd, "/data/outputs/", dfd, "/monthly/populations/explain-bias/hypergrid_lad.csv"))
  } else if (dfd == "fb_tts") {
  write_csv(results, paste0(wd, "/data/outputs/fb/census-month/populations/tts/explain-bias/hypergrid_lad.csv"))
  } else if (dfd == "fb_stt") {
  write_csv(results, paste0(wd, "/data/outputs/fb/census-month/populations/stt/explain-bias/hypergrid_lad.csv"))
  } else if (dfd == "mapp1") {
  write_csv(results, paste0(wd, "/data/outputs/mapp1/census-month/populations/explain-bias/hypergrid_lad.csv"))
  } else if (dfd == "mapp2") {
  write_csv(results, paste0(wd, "/data/outputs/mapp2/census-month/populations/explain-bias/hypergrid_lad.csv"))
  }



```

## Final model

Selecting optimal parameters

```{r}

if (dfd == "twitter"){
  hyper_grid <- read_csv(paste0(wd, "/data/outputs/", dfd, "/monthly/populations/explain-bias/hypergrid_lad.csv"))
  } else if (dfd == "fb_tts") {
  hyper_grid <- read_csv(paste0(wd, "/data/outputs/fb/census-month/populations/tts/explain-bias/hypergrid_lad.csv"))
  } else if (dfd == "fb_stt") {
  hyper_grid <- read_csv(paste0(wd, "/data/outputs/fb/census-month/populations/stt/explain-bias/hypergrid_lad.csv"))  
  } else if (dfd == "mapp1") {
  hyper_grid <- read_csv(paste0(wd, "/data/outputs/mapp1/census-month/populations/explain-bias/hypergrid_lad.csv"))  
  } else if (dfd == "mapp2") {
  hyper_grid <- read_csv(paste0(wd, "/data/outputs/mapp2/census-month/populations/explain-bias/hypergrid_lad.csv"))  
  }

optimal_pars <- hyper_grid %>% 
   slice_min(order_by = min_RMSE)
```

Once you've found the optimal model, we can fit our final model

```{r}
# parameter list
params <- list(
  eta = optimal_pars$eta,
  max_depth = optimal_pars$max_depth,
  min_child_weight = optimal_pars$min_child_weight,
  subsample = optimal_pars$subsample,
  colsample_bytree = optimal_pars$colsample_bytree,
  lambda = optimal_pars$lambda,
  alpha = optimal_pars$alpha
)

# train final model
xgb.fit.final <- xgboost(
  params = params,
  data = features_train,
  label = response_train,
  nrounds = 2000,
  objective = "reg:squarederror",
  verbose = 0,
  early_stopping_rounds = 15)
```

Save optimal model

```{r}

if (dfd == "twitter"){
  saveRDS(xgb.fit.final, paste0(wd, "/data/outputs/", dfd, "/monthly/populations/explain-bias/xgb-fit-final-lad.rds"))
  } else if (dfd == "fb_tts") {
  saveRDS(xgb.fit.final, paste0(wd, "/data/outputs/fb/census-month/populations/tts/explain-bias/xgb-fit-final-lad.rds"))
  } else if (dfd == "fb_stt") {
  saveRDS(xgb.fit.final, paste0(wd, "/data/outputs/fb/census-month/populations/stt/explain-bias/xgb-fit-final-lad.rds"))
  } else if (dfd == "mapp1") {
  saveRDS(xgb.fit.final, paste0(wd, "/data/outputs/mapp1/census-month/populations/explain-bias/xgb-fit-final-lad.rds"))
  } else if (dfd == "mapp2") {
  saveRDS(xgb.fit.final, paste0(wd, "/data/outputs/mapp2/census-month/populations/explain-bias/xgb-fit-final-lad.rds"))
  }
```

Extract parameters used for the final model

```{r}

write_csv(optimal_pars[1, ], paste0("../outputs/preliminary/",dfd,"/explain-bias/optimal_parameters.csv"))
```

# Visualising

## Model performance

```{r}
# Training sample
## predicted flow
aux_train_sample <- setNames(train_sample[ ,3:ncol(train_sample)], xgb.fit.final$feature_names)
aux_train_sample <- aux_train_sample[, !is.na(names(aux_train_sample))]
  
train_sample$bias_prediction <- predict(xgb.fit.final, as.matrix(aux_train_sample))
## compute rmse (or rmsd)
train_sample_rmse <- sqrt( sum( (train_sample$bias - train_sample$bias_prediction)^2 ) / ncol(train_sample) )
## pearson correlation
train_sample_pearson_cor <- cor(train_sample$bias, train_sample$bias_prediction, method = "pearson")
## spearman correlation
train_sample_spearman_cor <- cor(train_sample$bias, train_sample$bias_prediction, method = "spearman")
## standard deviation
train_sample_sd_error <- sd(train_sample$bias - train_sample$bias_prediction)

# Test sample
## predicted flow
aux_test_sample <- setNames(test_sample[ ,3:ncol(test_sample)], xgb.fit.final$feature_names)
aux_test_sample <- aux_test_sample[, !is.na(names(aux_test_sample))]

test_sample$bias_prediction <- predict(xgb.fit.final, as.matrix(aux_test_sample))
## compute rmse (or rmsd)
test_sample_rmse <- sqrt( sum( (test_sample$bias - test_sample$bias_prediction)^2 ) / ncol(test_sample) )
## pearson correlation
test_sample_pearson_cor <- cor(test_sample$bias, test_sample$bias_prediction, method = "pearson")
## spearman correlation
test_sample_spearman_cor <- cor(test_sample$bias, test_sample$bias_prediction, method = "spearman")
## standard deviation
test_sample_sd_error <- sd(test_sample$bias - test_sample$bias_prediction)
```

```{r}
train_sample_metrics <- c(train_sample_rmse, train_sample_pearson_cor, train_sample_spearman_cor, train_sample_sd_error) %>% 
  round(3)
test_sample_metrics <- c(test_sample_rmse, test_sample_pearson_cor, test_sample_spearman_cor, test_sample_sd_error) %>% 
  round(3)
metric <- c("rmse", "pearson", "spearman", "standard_dev")
data_source <- rep(dfd, each = length(test_sample_metrics))
summary_table <- data.frame(metric, train_sample_metrics, test_sample_metrics, data_source) 
names(summary_table) <- c("metric", "train", "test", "data_source")
summary_table

write_csv(summary_table, paste0("../outputs/preliminary/",dfd,"/explain-bias/model-performance.csv"))
```

## Variable importance

```{r}
# create importance matrix
importance_matrix <- xgb.importance(model = xgb.fit.final)

# obtain number of clusters
aux_p <- left_join(importance_matrix, df_var_lbl, join_by( Feature == var_label0)) %>% 
  dplyr::select( c(Gain, Cover, Frequency, var_label2) ) %>% 
  rename(Feature = "var_label2") %>% 
  xgb.ggplot.importance(., top_n = 30, measure = "Gain", xlab = "Relative importance")
num_clusters <- max(aux_p$data$Cluster)

# create plot
left_join(importance_matrix, df_var_lbl, join_by( Feature == var_label0)) %>% 
  dplyr::select( c(Gain, Cover, Frequency, var_label2) ) %>% 
  rename(Feature = "var_label2") %>% 
  xgb.ggplot.importance(., top_n = 30, measure = "Gain", xlab = "Relative importance") + 
  scale_fill_viridis_d(name = "Cluster",
                       pal_viridis()(2),
                       option = "C",
                       direction = 1) +
  #scale_fill_manual(values=c("#999999", "#E69F00")) +
  #labs(fill = "Clusters") +
  theme_plot_tufte() +
  theme(
    plot.title = element_text(size = 20),
    legend.text=element_text(size=14),
    legend.title=element_text(size=16),
    axis.title.x = element_text(size = 18),
    axis.title.y = element_text(size = 18),
    panel.background = element_rect(fill = "grey85",
                                    colour = "grey85",
                                    size = 0.5, linetype = "solid")
  )
```

```{r}
png(paste0("../outputs/preliminary/",dfd,"/explain-bias/feature-importance_v2.png"), units="in", width=8, height=10, res=300)
   last_plot()
dev.off()
```

## SHAP

Here is a gentle introduction to [SHAP values](https://blog.datascienceheroes.com/how-to-interpret-shap-values-in-r/).

Use of shapviz package <https://modeloriented.github.io/shapviz/>

Mean scores

```{r}
# Return the SHAP values and ranked features by mean|SHAP|
shap_values <- shap.values(xgb_model = xgb.fit.final, X_train = features_train)
# Ranked features by mean |SHAP|
# shap_values$mean_shap_score

shp <- shapviz(xgb.fit.final, X_pred = features_train, X = features_train)
```

**SHAP plots**

Bar plot - importance

```{r}
feature_lbl <- sv_importance(shp, kind = "bee", 
                             max_display = 20L) %>% 
  .$data %>% 
  distinct(.$feature) %>% 
  rename(labels = ".$feature") %>% 
  left_join(., df_var_lbl, join_by(labels == var_label0))

# Extract data from sv_importance plot
p_data <- sv_importance(shp, kind = "bar", 
                        viridis_args = list(option = "viridis"),
                        max_display = 20L)$data

min_val <- min(p_data$value, na.rm = TRUE)
max_val <- max(p_data$value, na.rm = TRUE)

sv_importance(shp, kind = "bar", 
              viridis_args = list(option = "viridis"),
              max_display = 20L) +
  scale_y_discrete( labels = rev(feature_lbl$var_label2)) +
  scale_x_continuous(
    labels = label_scientific(digits = 2),
    breaks = pretty_breaks(n = 3),
    expand = expansion(mult = c(0.02, 0.02))  # small padding left/right
  ) + 
  labs(
    x = "Mean SHAP value",
    y = "Features"
  ) +
  theme_plot_tufte() +
  theme(axis.title = element_text(size = 26),
        axis.text.y = element_text(size = 20),
        axis.text.x = element_text(size = 20))
```

```{r}
png(paste0("../outputs/preliminary/",dfd,"/explain-bias/feature-importance.png"), units="in", width=8, height=10, res=300)
  last_plot()
dev.off()
```

### Extract data

```{r}
fi_data <- sv_importance(shp, kind = "bar", 
              viridis_args = list(option = "viridis"),
              max_display = 40L) %>% 
  .$data


write_csv(fi_data, paste0("../outputs/preliminary/",dfd,"/explain-bias/feature-importance-data.csv"))
```

```{r}
feature_lbl <- sv_importance(shp, kind = "bee", 
                             max_display = 20L) %>% 
  .$data %>% 
  distinct(.$feature) %>% 
  rename(labels = ".$feature") %>% 
  left_join(., df_var_lbl, join_by(labels == var_label0))

sv_importance(shp, 
              kind = "bee", 
              max_display = 20L,
              viridis_args = list(option = "viridis") ) +
  scale_y_discrete( labels = rev(feature_lbl$var_label2)) +
  scale_x_continuous(
    labels = label_scientific(digits = 2),
    breaks = pretty_breaks(n = 3),
    expand = expansion(mult = c(0.02, 0.02))  # small padding left/right
  ) + 
  theme_plot_tufte() +
  theme(
    axis.title = element_text(size = 26),
    axis.text.y = element_text(size = 20),
    axis.text.x = element_text(size = 20),
    legend.title = element_text(size = 15),
    legend.text = element_text(size = 15))
  
```

### **How to interpret the plot above**

-   The y-axis indicates the variable name, in order of importance from top to bottom. The value next to them is the mean SHAP value.

-   The x-axis indicates the SHAP value and indicates how much is the change in log-odds.

-   The colour encodes the variable value.

-   Each point represents a row from the original dataset.

Lighter colours indicate that high values of a variable are associated with negative or positive values on the outcome / target variable.

Save shap plot

```{r}
png(paste0("../outputs/preliminary/",dfd,"/explain-bias/shap-plot-bias.png"), units="in", width=8, height=10, res=300)
  last_plot()
dev.off()
```

## Partial dependence plots

Plots for selected variables

```{r}
# Prepare the long-format data:
shap_long <- shap.prep(xgb_model = xgb.fit.final, X_train = features_train)
# # is the same as: using given shap_contrib
shap_long <- shap.prep(shap_contrib = shap_values$shap_score, X_train = features_train)

# select variables
selected_data <- left_join(shap_long, head(feature_lbl, n = 8), 
                                join_by(variable == labels) ) %>% 
  dplyr::filter(!is.na(var_label2))

# select variables for plot
selected_variables <- unique(selected_data$var_label2)

# create a list to store individual plots
plot_list <- list()

# loop over each selected variable and create an individual plot
for (var in selected_variables) {
  # Filter data for the current variable
  data_subset <- selected_data %>% filter(var_label2 == var)
  
  # create a scatter plot for the current variable
  p <- ggplot(data_subset, aes(x = rfvalue, y = value, color = rfvalue)) +
    geom_smooth(color = "gray95", span = 0.3) +
    geom_point(alpha = 0.7, size = 3) +
    scale_colour_viridis_c(option = "D",
                           labels = label_number(scale_cut = cut_short_scale()),
                           guide = guide_colorbar(title.position="top")) +
  scale_y_continuous(
    labels = label_number(scale_cut = cut_short_scale(), accuracy = 0.001)
    ) +
    labs(title = var, x = "Feature value", y = "Shape value") +
    theme_plot_tufte() +
    theme(
    title = element_text(size = 16),
    legend.position = "none",
    text = element_text(size = 15)
    # legend.key.width = unit(2.5, "cm"), 
    # legend.key.height = unit(0.4, "cm"),
    # legend.text = element_text(size = 12)
  )
  
  # add the plot to the list
  plot_list[[var]] <- p
}

# combine all plots into a single layout
combined_plot <- wrap_plots(plot_list, ncol = 2)

# display combined plot
combined_plot
```

```{r}
png(paste0("../outputs/preliminary/",dfd,"/explain-bias/shap-dependence.png"), units="in", width=8, height=10, res=300)
  last_plot()
dev.off()
```

::: callout-note
### **How to interpret the plot above**

The plot shows how a variable is associated with changes in shap values i.e. whether high score of a variable is associated with a decrease, increase or no change in shap values. They also provide a way to identify non-linear relationships.
:::

## Create a heatmap of LDAs based on feature importance

```{r}
xgb.fit.final_all <- xgboost(
  params = params,
  data = as.matrix(input_df[, 4:ncol(input_df)-1]),
  label = input_df$bias,
  nrounds = 2000,
  objective = "reg:squarederror",
  verbose = 0,
  early_stopping_rounds = 15)
```

```{r}
# Return the SHAP values and ranked features by mean|SHAP|
shap_values_full <- shap.values(xgb_model = xgb.fit.final_all, 
                                X_train = as.matrix(input_df[, 4:ncol(input_df)-1]))

aux_full <- as.data.frame(shap_values_full$shap_score)

input_df_with_shap_values <- cbind(input_df[,1:2], aux_full) %>% 
  pivot_longer(
    cols = c(3:length(.)),
    names_to = "feature",
    values_to = "shap_value"
  ) %>% 
  left_join(., df_var_lbl, join_by(feature == var_label5 ))
```

```{r}
selected_areas <- c(
  "Birmingham",
  "Leeds",
  "Sheffield",
  "Bradford",
  "Liverpool",
  "Manchester",
  "Bristol, City of",
  "Kirklees",
  "Croydon",
  "Barnet",
  "Isles of Scilly",
  "City of London",
  "Teesdale",
  "Berwick-upon-Tweed",
  "Alnwick",
  "Somerset West and Taunton",
  "Oswestry",
  "Rutland",
  "Shropshire",
  "Christchurch",
  "Purbeck",
  "Melton",
  "West Devon",
  "Ryedale",
  "Richmondshire",
  "Eden",
  "Castle Morpeth",
  "Corby"
)

input_df_with_shap_values %>% 
  filter(geography %in% selected_areas ) %>% 
  filter(var_label2 %in% feature_lbl$var_label2  ) %>% 
  tidyplot(x = geography, 
           y = var_label2,
           color = shap_value) %>%
  add_heatmap(rotate_labels = 90) %>% 
  adjust_size(height = 100) %>% 
  adjust_x_axis_title("") %>% 
  adjust_y_axis_title("") %>% 
  add(ggplot2::theme(
    legend.position = "bottom",
    legend.title = element_text(size=10, face = "plain", hjust=0.5, lineheight=0.45,
          color="black"),
    legend.key.width = unit(0.9, "cm"), 
    legend.key.height = unit(0.35, "cm"),
    legend.text = element_text(size = 8)
  )) %>% 
  add(ggplot2::labs(fill = "Shap value"))
```

```{r}
selected_areas <- c(
  "Birmingham",
  "Leeds",
  "Sheffield",
  "Bradford",
  "Liverpool",
  "Manchester",
  "Bristol, City of",
  "Kirklees",
  "Croydon",
  "Barnet",
  "Isles of Scilly",
  "City of London",
  "Teesdale",
  "Berwick-upon-Tweed",
  "Alnwick",
  "Somerset West and Taunton",
  "Oswestry",
  "Rutland",
  "Shropshire",
  "Christchurch",
  "Purbeck",
  "Melton",
  "West Devon",
  "Ryedale",
  "Richmondshire",
  "Eden",
  "Castle Morpeth",
  "Corby"
)

# Ensure that var_label2 follows the same order as the bar chart
heatmap_labels <- feature_lbl$var_label2

input_df_with_shap_values %>% 
  filter(geography %in% selected_areas) %>% 
  filter(var_label2 %in% heatmap_labels) %>% 
  mutate(var_label2 = factor(var_label2, levels = rev(heatmap_labels))) %>%  # Ensure same order
  tidyplot(x = geography, 
           y = var_label2,  # Apply the consistent label order
           color = shap_value) %>%
  add_heatmap(rotate_labels = 90) %>% 
  adjust_size(height = 100) %>% 
  adjust_x_axis_title("") %>% 
  adjust_y_axis_title("") %>% 
  add(ggplot2::theme(
    legend.position = "bottom",
    legend.title = element_text(size=10, face = "plain", hjust=0.5, lineheight=0.45,
          color="black"),
    legend.key.width = unit(0.9, "cm"), 
    legend.key.height = unit(0.35, "cm"),
    legend.text = element_text(size = 8)
  )) %>% 
  add(ggplot2::labs(fill = "Shap value"))
```

```{r}
png(paste0("../outputs/preliminary/",dfd,"/explain-bias/heatmap_xxldas.png"), units="in", width=8, height=10, res=300, bg = "transparent")
  last_plot()
dev.off()
```
