---
title: "explain-bias"
format: html
editor: visual
---

# Installing / loading packages

First, clear workspace:

```{r}
rm(list=ls())
```

```{r}
wd <- "/Volumes/DEBIAS"
```

```{r}
#| warning: false
#| message: false
# library necessary packages
library(xgboost)
library(parallel)
library(recommenderlab)
library(doParallel)
library(tidyverse)
library(moments)
library(SHAPforxgboost)
library(shapviz)
library(pdp)
library(rsample)
library(foreach)
library(patchwork)

```

# Set theme

```{r}
#| include = FALSE
# clear memory and adds tufte templates for plots and maps
source("../code/style/data-visualisation_theme.R")
```

# Data

## Data import

This first line defines the data source to be analysed. We have three options:

\- `twitter`

\- `fb_stt`

\- `fb_tts`

```{r}
dfd <- "twitter"
```

```{r}
# Read in data
df <- read.csv(paste0(wd, "/data/inputs/census/census2021-ts/combined-data-ltla.csv")) 

if (dfd == "twitter") {
  df_bias <- read.csv(paste0(wd, "/data/outputs/twitter/monthly/populations/active-population-bias/active_population_bias_lad.csv")) %>% rename("geography.code" = "code") %>% dplyr::filter(name != "City of London")
  } else if (dfd == "fb_tts") {
  df_bias <- read.csv(paste0(wd, "/data/outputs/fb/census-month/populations/tts/active-population-bias/active_population_bias_lad.csv")) %>% rename("geography.code" = "code")
  } else if (dfd == "fb_stt") {
  df_bias <- read.csv(paste0(wd, "/data/outputs/fb/census-month/populations/stt/active-population-bias/active_population_bias_lad.csv")) %>% rename("geography.code" = "code")
  }
```

```{r}
df <- merge(df, df_bias[, c("geography.code", "bias")], by = "geography.code")
```

```{r}
print(which(is.na(df), arr.ind = TRUE))

df <- na.omit(df)
```

## 2.2 Setting up model

```{r}
# Proportion of time-series data to assign to training vs testing
train_prop <- 0.7

# Proportion of training data which should be used to train the hyper-parameters
train_sample_prop <- 0.7

# Proportion of training data which should be used to validate the hyper-parameters
test_sample_prop <- 1

# Should the training and testing date be cut randomly ("random") or temporally ("temporal")?
cut <- "random"

# Independent variables
xvars <- colnames(df[, 6:ncol(df)-1])

# Final model inputs
y <- "bias"
x <- c(xvars)
```

```{r}
xvars
```

## 3.2 Defining training and test data sets

```{r}
set.seed(123)
split_df <- initial_split(df, prop = train_sample_prop)
train_sample <- training(split_df)
test_sample  <- testing(split_df)
```

```{r}
# variable names
features <- xvars

# Create the treatment plan from the training data
treatplan <- vtreat::designTreatmentsZ(train_sample, features, verbose = FALSE)

# Get the "clean" variable names from the scoreFrame
new_vars <- treatplan %>%
  magrittr::use_series(scoreFrame) %>%        
  dplyr::filter(code %in% c("clean", "lev")) %>% 
  magrittr::use_series(varName)  

# Prepare the training data
features_train <- vtreat::prepare(treatplan, train_sample, varRestriction = new_vars) %>% as.matrix()
response_train <- train_sample[,y]

# Prepare the test data
features_test <- vtreat::prepare(treatplan, test_sample, varRestriction = new_vars) %>% as.matrix()
response_test <- test_sample[, y]

# dimensions of one-hot encoded data
dim(features_train)
dim(features_test)
```

# Training

Set up parameters for model. Use of default parameters to train a basic 10-fold cross validated XGBoost model with 1,000 trees. THe default parameters are:

-   learning rate (eta): 0.3
-   tree depth (max_depth): 6
-   minimum node size (min_child_weight): 1
-   percent of training data to sample for each tree (subsample --\> equivalent to gbm's bag.fraction): 100%
-   early_stopping_rounds = 10 \# stop if no improvement for 10 consecutive

```{r}
# reproducibility
set.seed(123)

xgb.fit1 <- xgb.cv(
  data = features_train,
  label = response_train,
  nrounds = 1000,
  nfold = 10,
  objective = "reg:squarederror",  # for regression models
  verbose = 0,               # silent,
#  early_stopping_rounds = 10
)
```

```{r}
# get number of trees that minimize error
xgb.fit1$evaluation_log %>%
  dplyr::summarise(
    ntrees.train = which(train_rmse_mean == min(train_rmse_mean))[1],
    rmse.train   = min(train_rmse_mean),
    ntrees.test  = which(test_rmse_mean == min(test_rmse_mean))[1],
    rmse.test   = min(test_rmse_mean),
  )

# plot error vs number trees
ggplot(xgb.fit1$evaluation_log) +
  geom_line(aes(iter, train_rmse_mean), color = "red") +
  geom_line(aes(iter, test_rmse_mean), color = "blue")
```

We can see that the training error continues to decrease to 778 trees where the RMSE nearly reaches 0.0009509059 ; however, the cross validated error reaches a minimum RMSE of 1650 with only 14 trees.

## Tuning

To tune the XGBoost model we pass parameters as a list object to the params argument. The most common parameters include:

-   eta:controls the learning rate
-   max_depth: tree depth
-   min_child_weight: minimum number of observations required in each terminal node
-   subsample: percent of training data to sample for each tree
-   colsample_bytrees: percent of columns to sample from for each tree

We perform a large search grid, and create our hyperparameter search grid along with columns to dump our results in.

```{r}
# create hyperparameter grid
hyper_grid <- expand.grid(
  #nrounds = c(50, 100, 150),          # Number of boosting rounds
  eta = c(.01, .05, .1, .3),           # Learning rate
  #gamma = c(0, 1, 5),                 # Minimum loss reduction
  max_depth = c(1, 3, 5, 7),           # Maximum depth of a tree
  min_child_weight = c(1, 3, 5, 7),    # Minimum sum of instance weight
  subsample = c(.65, .8, 1),           # Fraction of rows per tree
  colsample_bytree = c(.8, .9, 1),     # Fraction of columns per tree
  lambda = c(0, 0.5, 1),               # L2 regularization term
  alpha = c(0, 0.5, 1),                 # L1 regularization term
  optimal_trees = 0,               # a place to dump results
  min_RMSE = 0                     # a place to dump results
)

nrow(hyper_grid)
```

## Parallelised code

We apply a loop procedure to loop through and apply a XGBoost model for each hyperparameter combination and dump the results in the hyper_grid data frame. This is done in a paralellised way.

```{r}

# Measure the time taken for parallel grid search
time_taken <- system.time({
  
# Set up parallel backend with the number of cores (adjust according to your system)
num_cores <- detectCores() - 1  # to leave one core free
cl <- makeCluster(num_cores)
registerDoParallel(cl)

# Parallelized grid search
results <- foreach(i = 1:nrow(hyper_grid), .combine = rbind, .packages = "xgboost") %dopar% {
  
  # create parameter list
  params <- list(
    eta = hyper_grid$eta[i],
    #gamma = hyper_grid$gamma[i],
    max_depth = hyper_grid$max_depth[i],
    min_child_weight = hyper_grid$min_child_weight[i],
    subsample = hyper_grid$subsample[i],
    colsample_bytree = hyper_grid$colsample_bytree[i],
    lambda = hyper_grid$lambda[i],
    alpha = hyper_grid$alpha[i]
  )
  
  # reproducibility
  set.seed(123)
  
  # train model
  xgb.tune <- xgb.cv(
    params = params,
    data = features_train,
    label = response_train,
    nrounds = 5000,
    nfold = 10,
    objective = "reg:squarederror",  # for regression models
    verbose = 0,                     # silent,
    early_stopping_rounds = 10       # stop if no improvement for 10 consecutive trees
  )
  
  # collect the optimal number of trees and minimum RMSE
  optimal_trees <- which.min(xgb.tune$evaluation_log$test_rmse_mean)
  min_RMSE <- min(xgb.tune$evaluation_log$test_rmse_mean)
  
  # return as a row (with hyperparameters and results)
  return(data.frame(
    eta = hyper_grid$eta[i],
    #gamma = hyper_grid$gamma[i],
    max_depth = hyper_grid$max_depth[i],
    min_child_weight = hyper_grid$min_child_weight[i],
    subsample = hyper_grid$subsample[i],
    colsample_bytree = hyper_grid$colsample_bytree[i],
    lambda = hyper_grid$lambda[i],
    alpha = hyper_grid$alpha[i],
    optimal_trees = optimal_trees,
    min_RMSE = min_RMSE
  ))
}

# Convert to a data frame if not already (should be)
results <- as.data.frame(results)

# Stop the cluster after processing
stopCluster(cl)
registerDoSEQ()  # Reset back to sequential processing

# Display the results
print(results)

})

# Print the time taken
print(time_taken)
```

## Non-parallelised code

::: callout-warning
This may take a while so leave it running overnight!
:::

```{r}

# # Measure the time taken for parallel grid search
# time_taken <- system.time({
#   
# # grid search 
# for(i in 1:nrow(hyper_grid)) {
# 
#   # create parameter list
#   params <- list(
#     eta = hyper_grid$eta[i],
#     max_depth = hyper_grid$max_depth[i],
#     min_child_weight = hyper_grid$min_child_weight[i],
#     subsample = hyper_grid$subsample[i],
#     colsample_bytree = hyper_grid$colsample_bytree[i]
#   )
# #
# #   # reproducibility
#   set.seed(123)
# 
#   # train model
#   xgb.tune <- xgb.cv(
#     params = params,
#     data = features_train,
#     label = response_train,
#     nrounds = 5000,
#     nfold = 10,
#     objective = "reg:squarederror",  # for regression models
#     verbose = 0,               # silent,
#     early_stopping_rounds = 10 # stop if no improvement for 10 consecutive trees
#   )
# #
# #   # add min training error and trees to grid
#    hyper_grid$optimal_trees[i] <- which.min(xgb.tune$evaluation_log$test_rmse_mean)
#    hyper_grid$min_RMSE[i] <- min(xgb.tune$evaluation_log$test_rmse_mean)
# }
# 
# })
# 
# # Print the time taken
# print(time_taken)
# 
# hyper_grid %>%
#   dplyr::arrange(min_RMSE) %>%
#   head(10)
# 
# print(hyper_grid)
```

```{r}

if (dfd == "twitter"){
  write_csv(hyper_grid, paste0(wd, "/data/outputs/", dfd, "/monthly/populations/explain-bias/hypergrid_lad.csv"))
  } else if (dfd == "fb_tts") {
  write_csv(hyper_grid, paste0(wd, "/data/outputs/fb/census-month/populations/tts/explain-bias/hypergrid_lad.csv"))
  } else if (dfd == "fb_stt") {
  write_csv(hyper_grid, paste0(wd, "/data/outputs/fb/census-month/populations/stt/explain-bias/hypergrid_lad.csv"))
  }

```

## Final model

Selecting optimal parameters

```{r}

if (dfd == "twitter"){
  hyper_grid <- read_csv(paste0(wd, "/data/outputs/", dfd, "/monthly/populations/explain-bias/hypergrid_lad.csv"))
  } else if (dfd == "fb_tts") {
  hyper_grid <- read_csv(paste0(wd, "/data/outputs/fb/census-month/populations/tts/explain-bias/hypergrid_lad.csv"))
  } else if (dfd == "fb_stt") {
  hyper_grid <- read_csv(paste0(wd, "/data/outputs/fb/census-month/populations/stt/explain-bias/hypergrid_lad.csv"))  
  }

optimal_pars <- hyper_grid %>% 
   slice_min(order_by = min_RMSE)
```

Once you've found the optimal model, we can fit our final model

```{r}
# parameter list
params <- list(
  eta = optimal_pars$eta,
  max_depth = optimal_pars$max_depth,
  min_child_weight = optimal_pars$min_child_weight,
  subsample = optimal_pars$subsample,
  colsample_bytree = optimal_pars$colsample_bytree,
  lambda = optimal_pars$lambda,
  alpha = optimal_pars$alpha
)

# train final model
xgb.fit.final <- xgboost(
  params = params,
  data = features_train,
  label = response_train,
  nrounds = 2000,
  objective = "reg:squarederror",
  verbose = 0,
  early_stopping_rounds = 15)
```

Save optimal model

```{r}

if (dfd == "twitter"){
  saveRDS(xgb.fit.final, paste0(wd, "/data/outputs/", dfd, "/monthly/populations/explain-bias/xgb-fit-final-lad.rds"))
  } else if (dfd == "fb_tts") {
  saveRDS(xgb.fit.final, paste0(wd, "/data/outputs/fb/census-month/populations/tts/explain-bias/xgb-fit-final-lad.rds"))
  } else if (dfd == "fb_stt") {
  saveRDS(xgb.fit.final, paste0(wd, "/data/outputs/fb/census-month/populations/stt/explain-bias/xgb-fit-final-lad.rds"))
  }
```

# Visualising

## Variable importance

```{r}
# create importance matrix
importance_matrix <- xgb.importance(model = xgb.fit.final)

# variable importance plot
xgb.ggplot.importance(importance_matrix, top_n = 30, measure = "Gain") + 
  theme_plot_tufte()
```

```{r}

png(paste0("../outputs/preliminary/",dfd,"/explain-bias/feature-importance.png"), units="in", width=8, height=10, res=300)
  last_plot()
dev.off()
```

## Partial dependence plots

```{r}
# pdp <- xgb.fit.final %>%
#   pdp::partial(pred.var = "per_age_20_24", n.trees = 503, grid.resolution = 100, train = features_train) %>%
#   autoplot(rug = TRUE, train = features_train) +
#   #scale_y_continuous(labels = scales::dollar) +
#   ggtitle("PDP")
# 
# ice <- xgb.fit.final %>%
#   pdp::partial(pred.var = "total_residents", n.trees = 503, grid.resolution = 100, train = features_train, ice = TRUE) %>%
#   autoplot(rug = TRUE, train = features_train, alpha = .1, center = TRUE) +
#   #scale_y_continuous(labels = scales::dollar) +
#   ggtitle("ICE")
# 
# gridExtra::grid.arrange(pdp, ice, nrow = 1)
# 
# png(paste0(wd, "/data/outputs/", dfd, "/monthly/populations/explain-bias/pdp-total-residents.png"), units="in", width=8, height=10, res=300)
#   last_plot()
# dev.off()
```

```{r}
# pdp <- xgb.fit.final %>%
#   pdp::partial(pred.var = "total_hh", n.trees = 503, grid.resolution = 100, train = features_train) %>%
#   autoplot(rug = TRUE, train = features_train) +
#   scale_y_continuous(labels = scales::dollar) +
#   ggtitle("PDP")
# 
# ice <- xgb.fit.final %>%
#   pdp::partial(pred.var = "total_hh", n.trees = 503, grid.resolution = 100, train = features_train, ice = TRUE) %>%
#   autoplot(rug = TRUE, train = features_train, alpha = .1, center = TRUE) +
#   scale_y_continuous(labels = scales::dollar) +
#   ggtitle("ICE")
# 
# gridExtra::grid.arrange(pdp, ice, nrow = 1)
```

## SHAP

Here is a gentle introduction to [SHAP values](https://blog.datascienceheroes.com/how-to-interpret-shap-values-in-r/).

Use of shapviz package <https://modeloriented.github.io/shapviz/>

Mean scores

```{r}
# Return the SHAP values and ranked features by mean|SHAP|
shap_values <- shap.values(xgb_model = xgb.fit.final, X_train = features_train)
# Ranked features by mean |SHAP|
# shap_values$mean_shap_score

shp <- shapviz(xgb.fit.final, X_pred = features_train, X = features_train)
```

SHAP plots

```{r}
#sv_importance(shp, show_numbers = TRUE)
sv_importance(shp, kind = "bee") +
  theme_plot_tufte()
```

### **How to interpret the plot above**

-   The y-axis indicates the variable name, in order of importance from top to bottom. The value next to them is the mean SHAP value.

-   The x-axis indicates the SHAP value and indicates how much is the change in log-odds.

-   The colour encodes the variable value.

-   Each point represents a row from the original dataset.

Lighter colours indicate that high values of a variable are associated with negative or positive values on the outcome / target variable.

Save shap plot

```{r}
png(paste0("../outputs/preliminary/",dfd,"/explain-bias/shap-plot-bias.png"), units="in", width=8, height=10, res=300)
  last_plot()
dev.off()
```

```{r}
# g1 <- shap.plot.dependence(data_long = shap_long, x = 'per_hh_no_centralheat', 
#                            color_feature = "per_hh_no_centralheat")
# g1

name_vector <- importance_matrix[order(importance_matrix$Gain, decreasing = TRUE)] %>%
  head(., n=4) %>% 
  distinct(Feature)

#sv_dependence(shp, v = names(shp$X)[1:4] )

sv_dependence(shp, v = unique(name_vector$Feature), alpha = 0.8 ) +
  plot_layout(ncol = 1)
```

::: callout-note
### **How to interpret the plot above**

The plot shows how a variable is associated with changes in shap values i.e. whether high score of a variable is associated with a decrease, increase or no change in shap values. They also provide a way to identify non-linear relationships.
:::

```{r}
png(paste0("../outputs/preliminary/",dfd,"/explain-bias/shap-dependence.png"), units="in", width=8, height=10, res=300)
  last_plot()
dev.off()
```

```{r}
# Prepare the long-format data:
shap_long <- shap.prep(xgb_model = xgb.fit.final, X_train = features_train)
# # is the same as: using given shap_contrib
shap_long <- shap.prep(shap_contrib = shap_values$shap_score, X_train = features_train)
# # **SHAP summary plot**
shap.plot.summary(shap_long)
shap.plot.dependence(data_long = shap_long, x = 'per_age_20_24', color_feature = 'per_age_20_24', size0 = 1)
```
