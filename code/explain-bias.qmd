---
title: "explain-bias"
format: html
editor: visual
---

# Installing / loading packages

First, clear workspace:

```{r}
rm(list=ls())
```

```{r}
wd <- "/Volumes/rdm04/DEBIAS"
wd_local <- "/Users/carmen/Library/CloudStorage/OneDrive-TheUniversityofLiverpool/github/de-bias"
```

```{r}
#| warning: false
#| message: false
# library necessary packages
library(xgboost)
library(parallel)
library(recommenderlab)
library(doParallel)
library(tidyverse)
library(moments)
library(SHAPforxgboost)
library(pdp)
library(rsample)
library(DiagrammeR)

```

# Data

## Data import

```{r}
dfd <- "fb_stt"
```

```{r}
# Read in data
df <- read.csv(paste0(wd, "/data/inputs/census/census2021-ts/combined-data-ltla.csv")) 

if (dfd == "twitter") {
  df_bias <- read.csv(paste0(wd, "/data/outputs/twitter/monthly/populations/active-population-bias/active_population_bias_lad.csv")) %>% rename("geography.code" = "code")
  } else if (dfd == "fb_tts") {
  df_bias <- read.csv(paste0(wd, "/data/outputs/fb/census-week/populations/tts/active-population-bias/active_population_bias_lad.csv")) %>% rename("geography.code" = "code")
  } else if (dfd == "fb_stt") {
  df_bias <- read.csv(paste0(wd, "/data/outputs/fb/census-week/populations/stt/active-population-bias/active_population_bias_lad.csv")) %>% rename("geography.code" = "code")
  }
```

```{r}
df <- merge(df, df_bias[, c("geography.code", "bias")], by = "geography.code")
```

```{r}
print(which(is.na(df), arr.ind = TRUE))

df <- na.omit(df)
```

## Setting up model

```{r}
# Proportion of time-series data to assign to training vs testing
train_prop <- 0.7

# Proportion of training data which should be used to train the hyper-parameters
train_sample_prop <- 0.7

# Proportion of training data which should be used to validate the hyper-parameters
test_sample_prop <- 1

# Should the training and testing date be cut randomly ("random") or temporally ("temporal")?
cut <- "random"

# Independent variables
xvars <- colnames(df[, 6:ncol(df)-1])

# Final model inputs
y <- "bias"
x <- c(xvars)
```

```{r}
xvars
```

## Defining training and test data sets

```{r}
set.seed(123)
split_df <- initial_split(df, prop = train_sample_prop)
train_sample <- training(split_df)
test_sample  <- testing(split_df)
```

```{r}
# variable names
features <- setdiff(names(train_sample), y)

# Create the treatment plan from the training data
treatplan <- vtreat::designTreatmentsZ(train_sample, features, verbose = FALSE)

# Get the "clean" variable names from the scoreFrame
new_vars <- treatplan %>%
  magrittr::use_series(scoreFrame) %>%        
  dplyr::filter(code %in% c("clean", "lev")) %>% 
  magrittr::use_series(varName)  

# Prepare the training data
features_train <- vtreat::prepare(treatplan, train_sample, varRestriction = new_vars) %>% as.matrix()
response_train <- train_sample[,y]

# Prepare the test data
features_test <- vtreat::prepare(treatplan, test_sample, varRestriction = new_vars) %>% as.matrix()
response_test <- test_sample[, y]

# dimensions of one-hot encoded data
dim(features_train)
dim(features_test)
```

# Training

Set up parameters for model. Use of default parameters to train a basic 10-fold cross validated XGBoost model with 1,000 trees. THe default parameters are:

-   learning rate (eta): 0.3
-   tree depth (max_depth): 6
-   minimum node size (min_child_weight): 1
-   percent of training data to sample for each tree (subsample --\> equivalent to gbm's bag.fraction): 100%
-   early_stopping_rounds = 10 \# stop if no improvement for 10 consecutive

```{r}
# reproducibility
set.seed(123)

xgb.fit1 <- xgb.cv(
  data = features_train,
  label = response_train,
  nrounds = 1000,
  nfold = 10,
  objective = "reg:squarederror",  # for regression models
  verbose = 0,               # silent,
#  early_stopping_rounds = 10
)
```

```{r}
# get number of trees that minimize error
xgb.fit1$evaluation_log %>%
  dplyr::summarise(
    ntrees.train = which(train_rmse_mean == min(train_rmse_mean))[1],
    rmse.train   = min(train_rmse_mean),
    ntrees.test  = which(test_rmse_mean == min(test_rmse_mean))[1],
    rmse.test   = min(test_rmse_mean),
  )

# plot error vs number trees
ggplot(xgb.fit1$evaluation_log) +
  geom_line(aes(iter, train_rmse_mean), color = "red") +
  geom_line(aes(iter, test_rmse_mean), color = "blue")
```

We can see that the training error continues to decrease to 778 trees where the RMSE nearly reaches 0.0009509059 ; however, the cross validated error reaches a minimum RMSE of 1650 with only 14 trees.

## Tuning

To tune the XGBoost model we pass parameters as a list object to the params argument. The most common parameters include:

-   eta:controls the learning rate
-   max_depth: tree depth
-   min_child_weight: minimum number of observations required in each terminal node
-   subsample: percent of training data to sample for each tree
-   colsample_bytrees: percent of columns to sample from for each tree

We perform a large search grid, and create our hyperparameter search grid along with columns to dump our results in.

```{r}
# create hyperparameter grid
hyper_grid <- expand.grid(
  eta = c(.01, .05, .1, .3),
  max_depth = c(1, 3, 5, 7),
  min_child_weight = c(1, 3, 5, 7),
  subsample = c(.65, .8, 1), 
  colsample_bytree = c(.8, .9, 1),
  optimal_trees = 0,               # a place to dump results
  min_RMSE = 0                     # a place to dump results
)

nrow(hyper_grid)
```

We apply a loop procedure to loop through and apply a XGBoost model for each hyperparameter combination and dump the results in the hyper_grid data frame.

::: callout-warning
This may take a while so leave it running overnight!
:::

```{r}
# grid search 
for(i in 1:nrow(hyper_grid)) {

  # create parameter list
  params <- list(
    eta = hyper_grid$eta[i],
    max_depth = hyper_grid$max_depth[i],
    min_child_weight = hyper_grid$min_child_weight[i],
    subsample = hyper_grid$subsample[i],
    colsample_bytree = hyper_grid$colsample_bytree[i]
  )
#   
#   # reproducibility
  set.seed(123)

  # train model
  xgb.tune <- xgb.cv(
    params = params,
    data = features_train,
    label = response_train,
    nrounds = 5000,
    nfold = 10,
    objective = "reg:squarederror",  # for regression models
    verbose = 0,               # silent,
    early_stopping_rounds = 10 # stop if no improvement for 10 consecutive trees
  )
#   
#   # add min training error and trees to grid
   hyper_grid$optimal_trees[i] <- which.min(xgb.tune$evaluation_log$test_rmse_mean)
   hyper_grid$min_RMSE[i] <- min(xgb.tune$evaluation_log$test_rmse_mean)
 }

hyper_grid %>%
  dplyr::arrange(min_RMSE) %>%
  head(10)
```

```{r}

if (dfd == "twitter"){
  write_csv(hyper_grid, paste0(wd, "/data/outputs/twitter/monthly/populations/explain-bias/hypergrid_lad.csv"))
  } else if (dfd == "fb_tts") {
  write_csv(hyper_grid, paste0(wd, "/data/outputs/fb/census-week/populations/tts/explain-bias/hypergrid_lad.csv"))
  } else if (dfd == "fb_stt") {
  write_csv(hyper_grid, paste0(wd, "/data/outputs/fb/census-week/populations/stt/explain-bias/hypergrid_lad.csv"))
  }

```

## Final model

Selecting optimal parameters

```{r}
dfd="fb_stt"

if (dfd == "twitter"){
  hyper_grid <- read_csv(paste0(wd, "/data/outputs/twitter/monthly/populations/explain-bias/hypergrid_lad.csv"))
  } else if (dfd == "fb_tts") {
  hyper_grid <- read_csv(paste0(wd, "/data/outputs/fb/census-week/populations/tts/explain-bias/hypergrid_lad.csv"))
  } else if (dfd == "fb_stt") {
  hyper_grid <- read_csv(paste0(wd, "/data/outputs/fb/census-week/populations/stt/explain-bias/hypergrid_lad.csv"))
  }

optimal_pars <- hyper_grid %>% 
   slice_min(order_by = min_RMSE)
```

Once you've found the optimal model, we can fit our final model

```{r}
# parameter list
params <- list(
  eta = optimal_pars$eta,
  max_depth = optimal_pars$max_depth,
  min_child_weight = optimal_pars$min_child_weight,
  subsample = optimal_pars$subsample,
  colsample_bytree = optimal_pars$colsample_bytree                   
)

# train final model
xgb.fit.final <- xgboost(
  params = params,
  data = features_train,
  label = response_train,
  nrounds = 2000,
  objective = "reg:squarederror",
  verbose = 0,
  early_stopping_rounds = 15)
```

Save optimal model

```{r}

if (dfd == "twitter"){
  saveRDS(xgb.fit.final, paste0(wd, "/data/outputs/twitter/monthly/populations/explain-bias/xgb-fit-final-lad.rds"))
  } else if (dfd == "fb_tts") {
  saveRDS(xgb.fit.final, paste0(wd, "/data/outputs/fb/census-week/populations/tts/explain-bias/xgb-fit-final-lad.rds"))
  } else if (dfd == "fb_stt") {
  saveRDS(xgb.fit.final, paste0(wd, "/data/outputs/fb/census-week/populations/stt/explain-bias/xgb-fit-final-lad.rds"))
  }
```

# Visualising

## Variable importance

```{r}
# create importance matrix
importance_matrix <- xgb.importance(model = xgb.fit.final)

xgb.plot.importance(importance_matrix, top_n = 20, measure = "Gain")

if (dfd == "twitter"){
  png(paste0(wd_local, "/bias-detection/outputs/preliminary/explain-bias/twitter_importance_lad.png"))
  xgb.plot.importance(importance_matrix, top_n = 20, measure = "Gain")
  dev.off()
  } else if (dfd == "fb_tts") {
  png(paste0(wd_local, "/bias-detection/outputs/preliminary/explain-bias/fb_tts_importance_lad.png"))
  xgb.plot.importance(importance_matrix, top_n = 20, measure = "Gain")
  dev.off()
  } else if (dfd == "fb_stt") {
  png(paste0(wd_local, "/bias-detection/outputs/preliminary/explain-bias/fb_stt_importance_lad.png"))
  xgb.plot.importance(importance_matrix, top_n = 20, measure = "Gain")
  dev.off()
  }
```

## Partial dependence plots

```{r}
pdp <- xgb.fit.final %>%
  pdp::partial(pred.var = "total_residents", n.trees = 503, grid.resolution = 100, train = features_train) %>%
  autoplot(rug = TRUE, train = features_train) +
  scale_y_continuous(labels = scales::dollar) +
  ggtitle("PDP")

ice <- xgb.fit.final %>%
  pdp::partial(pred.var = "total_residents", n.trees = 503, grid.resolution = 100, train = features_train, ice = TRUE) %>%
  autoplot(rug = TRUE, train = features_train, alpha = .1, center = TRUE) +
  scale_y_continuous(labels = scales::dollar) +
  ggtitle("ICE")

gridExtra::grid.arrange(pdp, ice, nrow = 1)

png(paste0(wd, "/data/outputs/", dfd, "/monthly/populations/explain-bias/pdp-total-residents.png"), units="in", width=8, height=10, res=300)
  last_plot()
dev.off()
```

```{r}
pdp <- xgb.fit.final %>%
  pdp::partial(pred.var = "popden", n.trees = 503, grid.resolution = 100, train = features_train) %>%
  autoplot(rug = TRUE, train = features_train) +
  scale_y_continuous(labels = scales::dollar) +
  ggtitle("PDP")

ice <- xgb.fit.final %>%
  pdp::partial(pred.var = "popden", n.trees = 503, grid.resolution = 100, train = features_train, ice = TRUE) %>%
  autoplot(rug = TRUE, train = features_train, alpha = .1, center = TRUE) +
  scale_y_continuous(labels = scales::dollar) +
  ggtitle("ICE")

gridExtra::grid.arrange(pdp, ice, nrow = 1)
```

```{r}
pdp <- xgb.fit.final %>%
  pdp::partial(pred.var = "avedur", n.trees = 503, grid.resolution = 100, train = features_train) %>%
  autoplot(rug = TRUE, train = features_train) +
  scale_y_continuous(labels = scales::dollar) +
  ggtitle("PDP")

ice <- xgb.fit.final %>%
  pdp::partial(pred.var = "avedur", n.trees = 503, grid.resolution = 100, train = features_train, ice = TRUE) %>%
  autoplot(rug = TRUE, train = features_train, alpha = .1, center = TRUE) +
  scale_y_continuous(labels = scales::dollar) +
  ggtitle("ICE")

gridExtra::grid.arrange(pdp, ice, nrow = 1)
```

```{r}
pdp <- xgb.fit.final %>%
  pdp::partial(pred.var = "houseprice", n.trees = 503, grid.resolution = 100, train = features_train) %>%
  autoplot(rug = TRUE, train = features_train) +
  scale_y_continuous(labels = scales::dollar) +
  ggtitle("PDP")

ice <- xgb.fit.final %>%
  pdp::partial(pred.var = "houseprice", n.trees = 503, grid.resolution = 100, train = features_train, ice = TRUE) %>%
  autoplot(rug = TRUE, train = features_train, alpha = .1, center = TRUE) +
  scale_y_continuous(labels = scales::dollar) +
  ggtitle("ICE")

gridExtra::grid.arrange(pdp, ice, nrow = 1)
```

```{r}
pdp <- xgb.fit.final %>%
  pdp::partial(pred.var = "tripfreq", n.trees = 503, grid.resolution = 100, train = features_train) %>%
  autoplot(rug = TRUE, train = features_train) +
  scale_y_continuous(labels = scales::dollar) +
  ggtitle("PDP")

ice <- xgb.fit.final %>%
  pdp::partial(pred.var = "tripfreq", n.trees = 503, grid.resolution = 100, train = features_train, ice = TRUE) %>%
  autoplot(rug = TRUE, train = features_train, alpha = .1, center = TRUE) +
  scale_y_continuous(labels = scales::dollar) +
  ggtitle("ICE")

gridExtra::grid.arrange(pdp, ice, nrow = 1)
```

## SHAP

Mean scores

```{r}
# Return the SHAP values and ranked features by mean|SHAP|
shap_values <- shap.values(xgb_model = xgb.fit.final, X_train = features_train)
# Ranked features by mean |SHAP|
shap_values$mean_shap_score
```

SHAP plots

```{r}
# Prepare the long-format data:
shap_long <- shap.prep(xgb_model = xgb.fit.final, X_train = features_train)
# is the same as: using given shap_contrib
shap_long <- shap.prep(shap_contrib = shap_values$shap_score, X_train = features_train)
# **SHAP summary plot**
shap.plot.summary(shap_long)
```

Save shap plot

```{r}
png("/Users/franciscorowe/Dropbox/Francisco/Research/in_progress/beijing-bus-ridership/code/sstick2/eve-destination/shap-plot-eve-destination.png", units="in", width=8, height=10, res=300)
  last_plot()
dev.off()
```
